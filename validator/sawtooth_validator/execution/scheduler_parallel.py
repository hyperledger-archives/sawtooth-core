# Copyright 2016-2017 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ------------------------------------------------------------------------------

from ast import literal_eval
from threading import Condition
from collections import deque

from sawtooth_validator.protobuf.transaction_pb2 import TransactionHeader

from sawtooth_validator.execution.scheduler import BatchExecutionResult
from sawtooth_validator.execution.scheduler import TxnInformation
from sawtooth_validator.execution.scheduler import Scheduler
from sawtooth_validator.execution.scheduler import SchedulerIterator
from sawtooth_validator.execution.scheduler_exceptions import SchedulerError


class PredecessorTreeNode:
    def __init__(self, children=None, readers=None, writer=None):
        self.children = children if children is not None else {}
        self.readers = readers if readers is not None else []
        self.writer = writer

    def __repr__(self):
        retval = {}

        if self.readers:
            retval['readers'] = self.readers
        if self.writer is not None:
            retval['writer'] = self.writer
        if self.children:
            retval['children'] = \
                {k: literal_eval(repr(v)) for k, v in self.children.items()}

        return repr(retval)


class PredecessorTree:
    def __init__(self, token_size=2):
        self._token_size = token_size
        self._root = PredecessorTreeNode()

    def __repr__(self):
        return repr(self._root)

    def _tokenize_address(self, address):
        return [address[i:i + self._token_size]
                for i in range(0, len(address), self._token_size)]

    def _get(self, address, create=False):
        tokens = self._tokenize_address(address)

        node = self._root
        for token in tokens:
            if token in node.children:
                node = node.children[token]
            else:
                if not create:
                    return None
                child = PredecessorTreeNode()
                node.children[token] = child
                node = child

        return node

    def get(self, address):
        return self._get(address)

    def add_reader(self, address, reader):
        node = self._get(address, create=True)
        node.readers.append(reader)

    def set_writer(self, address, writer):
        node = self._get(address, create=True)
        node.readers = []
        node.writer = writer
        node.children = {}

    def find_write_predecessors(self, address):
        """Returns all predecessor transaction ids for a write of the provided
        address.

        Arguments:
            address (str): the radix address

        Returns: a set of transaction ids
        """
        # A write operation must be preceded by:
        #   - The "enclosing writer", which is the writer at the address or
        #     the nearest writer higher (closer to the root) in the tree.
        #   - The "enclosing readers", which are the readers at the address
        #     or higher in the tree.
        #   - The "children writers", which include all writers which are
        #     lower in the tree than the address.
        #   - The "children readers", which include all readers which are
        #     lower in the tree than the address.
        #
        # The enclosing writer must be added as it may have modified a node
        # which must not happen after the current write.
        #
        # Writers which are higher in the tree than the enclosing writer may
        # have modified a node at or under the given address.  However, we do
        # not need to include them here as they will have been considered a
        # predecessor to the enclosing writer.
        #
        # Enclosing readers must be included.  Technically, we only need to add
        # enclosing readers which occurred after the enclosing writer, since
        # the readers preceding the writer will have been considered a
        # predecessor of the enclosing writer.  However, with the current
        # data structure we can not determine the difference between readers
        # so we specify them all; this is mostly harmless as it will not change
        # the eventual sort order generated by the scheduler.
        #
        # Children readers must be added, since their reads must happen prior
        # to the write.

        tokens = self._tokenize_address(address)

        predecessors = set()

        # First, walk down from the root to the address, collecting all readers
        # and updating the enclosing_writer if needed.

        node = self._root
        enclosing_writer = node.writer  # possibly None

        # the readers at the root node will always be added
        predecessors.update(set(node.readers))

        for token in tokens:
            # If the address isn't on the tree, then there aren't any
            # predecessors below the node to worry about (because
            # there isn't anything at all), so return the predecessors
            # that have already been collected.
            if token not in node.children:
                if enclosing_writer is not None:
                    predecessors.add(enclosing_writer)
                return predecessors

            node = node.children[token]

            # add enclosing readers directly to predecessors
            predecessors.update(set(node.readers))

            if node.writer is not None:
                enclosing_writer = node.writer

        if enclosing_writer is not None:
            predecessors.add(enclosing_writer)

        # Next, descend down the tree starting at the address node and find
        # all children writers and readers.  Uses breadth first search.

        to_process = deque()
        to_process.extendleft(node.children.values())
        while to_process:
            node = to_process.pop()
            predecessors.update(node.readers)
            if node.writer is not None:
                predecessors.add(node.writer)
            to_process.extendleft(node.children.values())

        return predecessors

    def find_read_predecessors(self, address):
        """Returns all predecessor transaction ids for a read of the provided
        address.

        Arguments:
            address (str): the radix address

        Returns: a set of transaction ids
        """
        # A read operation must be preceded by:
        #   - The "enclosing writer", which is the writer at the address or
        #     the nearest writer higher (closer to the root) in the tree.
        #   - All "children writers", which include all writers which are
        #     lower in the tree than the address.
        #
        # The enclosing writer must be added as it is possible it updated the
        # contents stored at address.
        #
        # Writers which are higher in the tree than the enclosing writer may
        # have modified the address.  However, we do not need to include them
        # here as they will have been considered a predecessor to the enclosing
        # writer.
        #
        # Children writers must be included as they may have updated addresses
        # lower in the tree, and these writers will have always been preceded
        # by the enclosing writer.
        #
        # We do not need to add any readers, since a reader cannot impact the
        # value which we are reading.  The relationship is transitive, in that
        # this reader will also not impact the readers already recorded in the
        # tree.

        tokens = self._tokenize_address(address)

        predecessors = set()

        # First, walk down from the root to the address, updating the
        # enclosing_writer if needed.

        node = self._root
        enclosing_writer = node.writer  # possibly None

        for token in tokens:
            # If the address isn't on the tree, then there aren't any
            # predecessors below the node to worry about (because
            # there isn't anything at all), so return the predecessors
            # that have already been collected.
            if token not in node.children:
                if enclosing_writer is not None:
                    predecessors.add(enclosing_writer)
                return predecessors

            node = node.children[token]

            if node.writer is not None:
                enclosing_writer = node.writer

        if enclosing_writer is not None:
            predecessors.add(enclosing_writer)

        # Next, descend down the tree starting at the address node and find
        # all children writers.  Uses breadth first search.

        to_process = deque()
        to_process.extendleft(node.children.values())
        while to_process:
            node = to_process.pop()
            if node.writer is not None:
                predecessors.add(node.writer)
            to_process.extendleft(node.children.values())

        return predecessors


class TransactionExecutionResult:
    def __init__(self, is_valid, context_id=None, state_hash=None):
        if is_valid and context_id is None:
            raise ValueError(
                "There must be a context_id for valid transactions")
        if not is_valid and context_id is not None:
            raise ValueError(
                "context_id must be None for invalid transactions")
        if not is_valid and state_hash is not None:
            raise ValueError(
                "state_hash must be None for invalid transactions")

        self.is_valid = is_valid
        self.context_id = context_id
        self.state_hash = state_hash


class ParallelScheduler(Scheduler):
    def __init__(self, squash_handler, first_state_hash, always_persist):
        self._squash = squash_handler
        self._first_state_hash = first_state_hash
        self._last_state_hash = first_state_hash
        self._condition = Condition()
        self._predecessor_tree = PredecessorTree()
        self._txn_predecessors = {}

        self._always_persist = always_persist

        # Transaction identifiers which have been scheduled.  Stored as a list,
        # since order is important; SchedulerIterator instances, for example,
        # must all return scheduled transactions in the same order.
        self._scheduled = []

        # A dict of transaction id to TxnInformation objects, containing all
        # transactions present in self._scheduled.
        self._scheduled_txn_info = {}

        # All batches in their natural order (the order they were added to
        # the scheduler.
        self._batches = []
        # The batches that have state hashes added in add_batch, used in
        # Block validation.
        self._batches_with_state_hash = {}

        # Indexes to find a batch quickly
        self._batches_by_id = {}
        self._batches_by_txn_id = {}

        # Transaction results
        self._txn_results = {}

        self._cancelled = False
        self._final = False

    def _find_input_dependencies(self, inputs):
        """Use the predecessor tree to find dependencies based on inputs.

        Returns: A list of transaction ids.
        """
        dependencies = []
        for address in inputs:
            dependencies.extend(
                self._predecessor_tree.find_read_predecessors(address))
        return dependencies

    def _find_output_dependencies(self, outputs):
        """Use the predecessor tree to find dependencies based on outputs.

        Returns: A list of transaction ids.
        """
        dependencies = []
        for address in outputs:
            dependencies.extend(
                self._predecessor_tree.find_write_predecessors(address))
        return dependencies

    def add_batch(self, batch, state_hash=None):
        with self._condition:
            if self._final:
                raise SchedulerError('Invalid attempt to add batch to '
                                     'finalized scheduler; batch: {}'
                                     .format(batch.header_signature))

            self._batches.append(batch)
            self._batches_by_id[batch.header_signature] = batch
            for txn in batch.transactions:
                self._batches_by_txn_id[txn.header_signature] = batch

            if state_hash is not None:
                b_id = batch.header_signature
                self._batches_with_state_hash[b_id] = state_hash

            # For dependency handling: First, we determine our dependencies
            # based on the current state of the predecessor tree.  Second,
            # we update the predecessor tree with reader and writer
            # information based on input and outputs.
            for txn in batch.transactions:
                header = TransactionHeader()
                header.ParseFromString(txn.header)

                # Calculate predecessors (transaction ids which must come
                # prior to the current transaction).
                predecessors = self._find_input_dependencies(header.inputs)
                predecessors.extend(
                    self._find_output_dependencies(header.outputs))
                predecessors.extend(header.dependencies)

                # Update our internal state with the computed predecessors.
                self._txn_predecessors[txn.header_signature] = predecessors

                # Update the predecessor tree.
                #
                # Order of reader/writer operations is relevant.  A writer
                # may overshadow a reader.  For example, if the transaction
                # has the same input/output address, the end result will be
                # this writer (txn.header_signature) stored at the address of
                # the predecessor tree.  The reader information will have been
                # discarded.  Write operations to partial addresses will also
                # overshadow entire parts of the predecessor tree.
                #
                # Thus, the order here (inputs then outputs) will cause the
                # minimal amount of relevant information to be stored in the
                # predecessor tree, with duplicate information being
                # automatically discarded by the set_writer() call.
                for address in header.inputs:
                    self._predecessor_tree.add_reader(
                        address, txn.header_signature)
                for address in header.outputs:
                    self._predecessor_tree.set_writer(
                        address, txn.header_signature)

            self._condition.notify_all()

    def _get_batch_index(self, batch):
        for i, batch_i in enumerate(self._batches):
            if batch_i.header_signature == batch.header_signature:
                return i
        raise ValueError("no such batch: {}".format(batch.header_signature))

    def _is_explicit_request_for_state_root(self, batch_signature):
        return batch_signature in self._batches_with_state_hash

    def _is_implicit_request_for_state_root(self, batch_signature):
        return self._final and self._is_last_valid_batch(batch_signature)

    def _is_valid_batch(self, batch):
        for txn in batch.transactions:
            result = self._txn_results[txn.header_signature]
            if not result.is_valid:
                return False
        return True

    def _is_last_valid_batch(self, batch_signature):
        batch = self._batches_by_id[batch_signature]
        if not self._is_valid_batch(batch):
            return False
        index_of_next = self._batches.index(batch) + 1
        for later_batch in self._batches[index_of_next:]:
            if self._is_valid_batch(later_batch):
                return False
        return True

    def _get_contexts_for_squash(self, batch_signature):
        """Starting with the batch referenced by batch_signature, iterate back
        through the batches and for each valid batch collect the context_id.
        At the end remove contexts for txns that are other txn's predecessors.

        Args:
            batch_signature (str): The batch to start from, moving back through
                the batches in the scheduler

        Returns:
            (list): Context ids that haven't been previous base contexts.
        """

        batch = self._batches_by_id[batch_signature]
        index = self._batches.index(batch)
        contexts_by_txn_id = {}
        some_txns_predecessor = []
        for b in self._batches[index::-1]:
            batch_is_valid = True
            contexts_from_batch_by_txn_id = {}
            for txn in b.transactions[::-1]:
                result = self._txn_results[txn.header_signature]
                if not result.is_valid:
                    batch_is_valid = False
                    break
                else:
                    txn_id = txn.header_signature
                    contexts_from_batch_by_txn_id[txn_id] = result.context_id
            if batch_is_valid:
                for txn_id in contexts_from_batch_by_txn_id:
                    some_txns_predecessor.extend(
                        self._txn_predecessors[txn_id])
                contexts_by_txn_id.update(contexts_from_batch_by_txn_id)
        txn_ids_to_possibly_remove = list(contexts_by_txn_id.keys()).copy()
        for txn_id in txn_ids_to_possibly_remove:
            if txn_id in some_txns_predecessor:
                del contexts_by_txn_id[txn_id]
        return list(contexts_by_txn_id.values())

    def _is_state_hash_correct(self, state_hash, batch_id):
        return state_hash == self._batches_with_state_hash[batch_id]

    def get_batch_execution_result(self, batch_signature):
        with self._condition:
            # This method calculates the BatchExecutionResult on the fly,
            # where only the TransactionExecutionResults are cached, instead
            # of BatchExecutionResults, as in the SerialScheduler
            batch = self._batches_by_id[batch_signature]

            for txn in batch.transactions:
                txn_result = self._txn_results[txn.header_signature]
                if not txn_result.is_valid:
                    return BatchExecutionResult(
                        is_valid=False, state_hash=None)
            state_hash = None
            if self._is_explicit_request_for_state_root(batch_signature):
                contexts = self._get_contexts_for_squash(batch_signature)
                state_hash = self._squash(self._first_state_hash,
                                          contexts,
                                          persist=False,
                                          clean_up=False)
                if self._is_state_hash_correct(state_hash, batch_signature):
                    self._squash(self._first_state_hash,
                                 contexts,
                                 persist=True,
                                 clean_up=True)
                else:
                    self._squash(self._first_state_hash,
                                 contexts,
                                 persist=False,
                                 clean_up=True)
            elif self._is_implicit_request_for_state_root(batch_signature):
                contexts = self._get_contexts_for_squash(batch_signature)
                state_hash = self._squash(self._first_state_hash,
                                          contexts,
                                          persist=self._always_persist,
                                          clean_up=True)
            return BatchExecutionResult(is_valid=True, state_hash=state_hash)

    def set_transaction_execution_result(
            self, txn_signature, is_valid, context_id):
        with self._condition:
            if txn_signature not in self._scheduled:
                raise SchedulerError("transaction not scheduled: {}".format(
                    txn_signature))

            txn_result = TransactionExecutionResult(
                is_valid=is_valid,
                context_id=context_id if is_valid else None,
                state_hash=self._first_state_hash if is_valid else None)

            self._txn_results[txn_signature] = txn_result

            # mark all transactions which depend explicitly upon this one as
            # invalid as well

            self._condition.notify_all()

    def _unscheduled_transactions(self):
        # shouldn't actually return txn if dependencies were
        # marked invalid.
        txns = []
        for batch in self._batches:
            for txn in batch.transactions:
                if txn.header_signature not in self._scheduled:
                    txns.append(txn)
        return txns

    def _has_predecessors(self, txn):
        for predecessor_id in self._txn_predecessors[txn.header_signature]:
            if predecessor_id not in self._txn_results:
                return True

        return False

    def _txn_result_is_invalid(self, sig):
        return sig in self._txn_results and \
               not self._txn_results[sig].is_valid

    def _txn_is_in_valid_batch(self, txn_id):
        """Returns whether the transaction is in a valid batch.

        Args:
            txn_id (str): The transaction header signature.

        Returns:
            (bool): True if the txn's batch is valid, False otherwise.
        """

        batch = self._batches_by_txn_id[txn_id]
        for txn in batch.transactions:
            if self._txn_result_is_invalid(sig=txn.header_signature):
                return False
        return True

    def _predecessor_not_predecessor_of_predecessors(self,
                                                     prior_txn_id,
                                                     predecessors):
        """

        Args:
            prior_txn_id (str): The predecessor's txn header_signature.
            predecessors (list): The predecessors of the txn that the
                prior_txn_id is a predecessor of.

        Returns:
            (bool): The prior_txn_id is not a predecessor of a predecessor.
        """

        for pred_id in predecessors:
            if prior_txn_id in self._txn_predecessors[pred_id]:
                return False
        return True

    def _get_initial_state_for_transaction(self, txn):
        # Collect contexts that this transaction depends upon
        # We assume that all prior txns in the batch are valid
        # or else this transaction wouldn't run. Also any explicit
        # dependencies that could have failed this txn did so.
        contexts = []
        predecessors = self._txn_predecessors[txn.header_signature]
        for prior_txn_id in predecessors:
            if self._txn_is_in_valid_batch(prior_txn_id):
                result = self._txn_results[prior_txn_id]
                if self._predecessor_not_predecessor_of_predecessors(
                        prior_txn_id,
                        predecessors):
                    contexts.append(result.context_id)
        return contexts

    def next_transaction(self):
        with self._condition:
            # We return the next transaction which hasn't been scheduled and
            # is not blocked by a dependency.

            next_txn = None
            for txn in self._unscheduled_transactions():
                if not self._has_predecessors(txn):
                    next_txn = txn
                    break

            if next_txn is not None:
                bases = self._get_initial_state_for_transaction(next_txn)

                info = TxnInformation(
                    txn=next_txn,
                    state_hash=self._first_state_hash,
                    base_context_ids=bases)
                self._scheduled.append(next_txn.header_signature)
                self._scheduled_txn_info[next_txn.header_signature] = info
                return info
            return None

    def available(self):
        with self._condition:
            # We return the next transaction which hasn't been scheduled and
            # is not blocked by a dependency.

            count = 0
            for txn in self._unscheduled_transactions():
                if not self._has_predecessors(txn):
                    count += 1

            return count

    def finalize(self):
        with self._condition:
            self._final = True
            self._condition.notify_all()

    def _complete(self):
        return self._final and \
                    len(self._txn_results) == len(self._batches_by_txn_id)

    def complete(self, block=True):
        with self._condition:
            if self._complete():
                return True
            if block:
                self._condition.wait_for(self._complete)
            else:
                return False

    def __del__(self):
        self.cancel()

    def __iter__(self):
        return SchedulerIterator(self, self._condition)

    def count(self):
        with self._condition:
            return len(self._scheduled)

    def get_transaction(self, index):
        with self._condition:
            return self._scheduled_txn_info[self._scheduled[index]]

    def cancel(self):
        with self._condition:
            self._cancelled = True
            self._condition.notify_all()

    def is_cancelled(self):
        with self._condition:
            return self._cancelled
