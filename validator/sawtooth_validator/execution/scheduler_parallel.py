# Copyright 2016-2017 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ------------------------------------------------------------------------------

from ast import literal_eval
from threading import Condition
from collections import deque

from sawtooth_validator.protobuf.transaction_pb2 import TransactionHeader

from sawtooth_validator.execution.scheduler import BatchExecutionResult
from sawtooth_validator.execution.scheduler import TxnInformation
from sawtooth_validator.execution.scheduler import Scheduler
from sawtooth_validator.execution.scheduler import SchedulerIterator
from sawtooth_validator.execution.scheduler_exceptions import SchedulerError


class PredecessorTreeNode:
    def __init__(self, children=None, readers=None, writer=None):
        self.children = children if children is not None else {}
        self.readers = readers if readers is not None else []
        self.writer = writer

    def __repr__(self):
        retval = {}

        if self.readers:
            retval['readers'] = self.readers
        if self.writer is not None:
            retval['writer'] = self.writer
        if self.children:
            retval['children'] = \
                {k: literal_eval(repr(v)) for k, v in self.children.items()}

        return repr(retval)


class PredecessorTree:
    def __init__(self, token_size=2):
        self._token_size = token_size
        self._root = PredecessorTreeNode()

    def __repr__(self):
        return repr(self._root)

    def _tokenize_address(self, address):
        return [address[i:i + self._token_size]
                for i in range(0, len(address), self._token_size)]

    def _get(self, address, create=False):
        tokens = self._tokenize_address(address)

        node = self._root
        for token in tokens:
            if token in node.children:
                node = node.children[token]
            else:
                if not create:
                    return None
                child = PredecessorTreeNode()
                node.children[token] = child
                node = child

        return node

    def get(self, address):
        return self._get(address)

    def add_reader(self, address, reader):
        node = self._get(address, create=True)
        node.readers.append(reader)

    def set_writer(self, address, writer):
        node = self._get(address, create=True)
        node.readers = []
        node.writer = writer
        node.children = {}

    def find_write_predecessors(self, address):
        """Returns all predecessor transaction ids for a write of the provided
        address.

        Arguments:
            address (str): the radix address

        Returns: a set of transaction ids
        """
        # A write operation must be preceded by:
        #   - The "enclosing writer", which is the writer at the address or
        #     the nearest writer higher (closer to the root) in the tree.
        #   - The "enclosing readers", which are the readers at the address
        #     or higher in the tree.
        #   - The "children writers", which include all writers which are
        #     lower in the tree than the address.
        #   - The "children readers", which include all readers which are
        #     lower in the tree than the address.
        #
        # The enclosing writer must be added as it may have modified a node
        # which must not happen after the current write.
        #
        # Writers which are higher in the tree than the enclosing writer may
        # have modified a node at or under the given address.  However, we do
        # not need to include them here as they will have been considered a
        # predecessor to the enclosing writer.
        #
        # Enclosing readers must be included.  Technically, we only need to add
        # enclosing readers which occurred after the enclosing writer, since
        # the readers preceding the writer will have been considered a
        # predecessor of the enclosing writer.  However, with the current
        # data structure we can not determine the difference between readers
        # so we specify them all; this is mostly harmless as it will not change
        # the eventual sort order generated by the scheduler.
        #
        # Children readers must be added, since their reads must happen prior
        # to the write.

        tokens = self._tokenize_address(address)

        predecessors = set()

        # First, walk down from the root to the address, collecting all readers
        # and updating the enclosing_writer if needed.

        node = self._root
        enclosing_writer = node.writer  # possibly None

        # the readers at the root node will always be added
        predecessors.update(set(node.readers))

        for token in tokens:
            # If the address isn't on the tree, then there aren't any
            # predecessors below the node to worry about (because
            # there isn't anything at all), so return the predecessors
            # that have already been collected.
            if token not in node.children:
                if enclosing_writer is not None:
                    predecessors.add(enclosing_writer)
                return predecessors

            node = node.children[token]

            # add enclosing readers directly to predecessors
            predecessors.update(set(node.readers))

            if node.writer is not None:
                enclosing_writer = node.writer

        if enclosing_writer is not None:
            predecessors.add(enclosing_writer)

        # Next, descend down the tree starting at the address node and find
        # all children writers and readers.  Uses breadth first search.

        to_process = deque()
        to_process.extendleft(node.children.values())
        while to_process:
            node = to_process.pop()
            predecessors.update(node.readers)
            if node.writer is not None:
                predecessors.add(node.writer)
            to_process.extendleft(node.children.values())

        return predecessors

    def find_read_predecessors(self, address):
        """Returns all predecessor transaction ids for a read of the provided
        address.

        Arguments:
            address (str): the radix address

        Returns: a set of transaction ids
        """
        # A read operation must be preceded by:
        #   - The "enclosing writer", which is the writer at the address or
        #     the nearest writer higher (closer to the root) in the tree.
        #   - All "children writers", which include all writers which are
        #     lower in the tree than the address.
        #
        # The enclosing writer must be added as it is possible it updated the
        # contents stored at address.
        #
        # Writers which are higher in the tree than the enclosing writer may
        # have modified the address.  However, we do not need to include them
        # here as they will have been considered a predecessor to the enclosing
        # writer.
        #
        # Children writers must be included as they may have updated addresses
        # lower in the tree, and these writers will have always been preceded
        # by the enclosing writer.
        #
        # We do not need to add any readers, since a reader cannot impact the
        # value which we are reading.  The relationship is transitive, in that
        # this reader will also not impact the readers already recorded in the
        # tree.

        tokens = self._tokenize_address(address)

        predecessors = set()

        # First, walk down from the root to the address, updating the
        # enclosing_writer if needed.

        node = self._root
        enclosing_writer = node.writer  # possibly None

        for token in tokens:
            # If the address isn't on the tree, then there aren't any
            # predecessors below the node to worry about (because
            # there isn't anything at all), so return the predecessors
            # that have already been collected.
            if token not in node.children:
                if enclosing_writer is not None:
                    predecessors.add(enclosing_writer)
                return predecessors

            node = node.children[token]

            if node.writer is not None:
                enclosing_writer = node.writer

        if enclosing_writer is not None:
            predecessors.add(enclosing_writer)

        # Next, descend down the tree starting at the address node and find
        # all children writers.  Uses breadth first search.

        to_process = deque()
        to_process.extendleft(node.children.values())
        while to_process:
            node = to_process.pop()
            if node.writer is not None:
                predecessors.add(node.writer)
            to_process.extendleft(node.children.values())

        return predecessors


class TransactionExecutionResult:
    def __init__(self, is_valid, context_id=None, state_hash=None):
        if context_id is not None and state_hash is not None:
            raise ValueError(
                "context_id and state_hash are exclusive arguments")
        if not is_valid and context_id is not None:
            raise ValueError(
                "context_id must be None for invalid transactions")
        if not is_valid and state_hash is not None:
            raise ValueError(
                "state_hash must be None for invalid transactions")

        self.is_valid = is_valid
        self.context_id = context_id
        self.state_hash = state_hash


class ParallelScheduler(Scheduler):
    def __init__(self, squash_handler, first_state_hash):
        self._squash = squash_handler
        self._first_state_hash = first_state_hash
        self._last_state_hash = first_state_hash
        self._condition = Condition()
        self._predecessor_tree = PredecessorTree()
        self._txn_predecessors = {}

        # Transaction identifiers which have been scheduled.  Stored as a list,
        # since order is important; SchedulerIterator instances, for example,
        # must all return scheduled transactions in the same order.
        self._scheduled = []

        # A dict of transaction id to TxnInformation objects, containing all
        # transactions present in self._scheduled.
        self._scheduled_txn_info = {}

        # All batches in their natural order (the order they were added to
        # the scheduler.
        self._batches = []

        # Indexes to find a batch quickly
        self._batches_by_id = {}
        self._batches_by_txn_id = {}

        # Transaction results
        self._txn_results = {}

        self._cancelled = False
        self._final = False

    def _find_input_dependencies(self, inputs):
        """Use the predecessor tree to find dependencies based on inputs.

        Returns: A list of transaction ids.
        """
        dependencies = []
        for address in inputs:
            dependencies.extend(
                self._predecessor_tree.find_read_predecessors(address))
        return dependencies

    def _find_output_dependencies(self, outputs):
        """Use the predecessor tree to find dependencies based on outputs.

        Returns: A list of transaction ids.
        """
        dependencies = []
        for address in outputs:
            dependencies.extend(
                self._predecessor_tree.find_write_predecessors(address))
        return dependencies

    def add_batch(self, batch, state_hash=None):
        with self._condition:
            if self._final:
                raise SchedulerError('Invalid attempt to add batch to '
                                     'finalized scheduler; batch: {}'
                                     .format(batch.header_signature))

            self._batches.append(batch)
            self._batches_by_id[batch.header_signature] = batch
            for txn in batch.transactions:
                self._batches_by_txn_id[txn.header_signature] = batch

            # For dependency handling: First, we determine our dependencies
            # based on the current state of the predecessor tree.  Second,
            # we update the predecessor tree with reader and writer
            # information based on input and outputs.
            for txn in batch.transactions:
                header = TransactionHeader()
                header.ParseFromString(txn.header)

                # Calculate predecessors (transaction ids which must come
                # prior to the current transaction).
                predecessors = self._find_input_dependencies(header.inputs)
                predecessors.extend(
                    self._find_output_dependencies(header.outputs))
                predecessors.extend(header.dependencies)

                # Update our internal state with the computed predecessors.
                self._txn_predecessors[txn.header_signature] = predecessors

                # Update the predecessor tree.
                #
                # Order of reader/writer operations is relevant.  A writer
                # may overshadow a reader.  For example, if the transaction
                # has the same input/output address, the end result will be
                # this writer (txn.header_signature) stored at the address of
                # the predecessor tree.  The reader information will have been
                # discarded.  Write operations to partial addresses will also
                # overshadow entire parts of the predecessor tree.
                #
                # Thus, the order here (inputs then outputs) will cause the
                # minimal amount of relevant information to be stored in the
                # predecessor tree, with duplicate information being
                # automatically discarded by the set_writer() call.
                for address in header.inputs:
                    self._predecessor_tree.add_reader(
                        address, txn.header_signature)
                for address in header.outputs:
                    self._predecessor_tree.set_writer(
                        address, txn.header_signature)

            self._condition.notify_all()

    def _get_batch_index(self, batch):
        for i, batch_i in enumerate(self._batches):
            if batch_i.header_signature == batch.header_signature:
                return i
        raise ValueError("no such batch: {}".format(batch.header_signature))

    def get_batch_execution_result(self, batch_signature):
        with self._condition:
            batch = self._batches_by_id[batch_signature]

            last_state_hash = None
            contexts = []
            for txn in batch.transactions:
                txn_result = self._txn_results[txn.header_signature]
                if not txn_result.is_valid:
                    return BatchExecutionResult(
                        is_valid=False, state_hash=None)
                if txn_result.context_id is not None:
                    contexts.append(txn_result.context_id)
                if txn_result.state_hash is not None:
                    last_state_hash = txn_result.state_hash

            if last_state_hash is None:
                # Find the index of the batch
                index = self._get_batch_index(batch)
                if index == 0:
                    last_state_hash = self._first_state_hash
                else:
                    prev_batch = self._batches[index - 1]
                    last_state_hash = \
                        self._batch_results[prev_batch].state_hash

            state_hash = self._squash(last_state_hash, contexts, persist=True,
                                      clean_up=False)
            return BatchExecutionResult(is_valid=True, state_hash=state_hash)

    def set_transaction_execution_result(
            self, txn_signature, is_valid, context_id):
        with self._condition:
            if txn_signature not in self._scheduled:
                raise SchedulerError("transaction not scheduled: {}".format(
                    txn_signature))

            txn_result = TransactionExecutionResult(
                is_valid=is_valid,
                context_id=context_id)

            self._txn_results[txn_signature] = txn_result

            # mark all transactions which depend upon this one as
            # invalid as well

            # mark all transactions in batches impacted by this failure
            # as failed -- "failed by association"; this presumably trickles
            # through and invalidates a lot

            self._condition.notify_all()

    def _unscheduled_transactions(self):
        # shouldn't actually return txn if dependencies were
        # marked invalid.
        txns = []
        for batch in self._batches:
            for txn in batch.transactions:
                if txn.header_signature not in self._scheduled:
                    txns.append(txn)
        return txns

    def _has_predecessors(self, txn):
        for predecessor_id in self._txn_predecessors[txn.header_signature]:
            if predecessor_id not in self._txn_results:
                return True

        return False

    def _get_initial_state_for_transaction(self, txn):
        # Collect contexts that this transaction depends upon
        return self._last_state_hash

    def next_transaction(self):
        with self._condition:
            # We return the next transaction which hasn't been scheduled and
            # is not blocked by a dependency.

            next_txn = None
            for txn in self._unscheduled_transactions():
                if not self._has_predecessors(txn):
                    next_txn = txn
                    break

            if next_txn is not None:
                state_hash = self._get_initial_state_for_transaction(next_txn)

                info = TxnInformation(
                    txn=next_txn,
                    state_hash=state_hash,
                    base_context_ids=[])
                self._scheduled.append(next_txn.header_signature)
                self._scheduled_txn_info[next_txn.header_signature] = info
                return info
            else:
                return None

    def available(self):
        with self._condition:
            # We return the next transaction which hasn't been scheduled and
            # is not blocked by a dependency.

            count = 0
            for txn in self._unscheduled_transactions():
                if not self._has_predecessors(txn):
                    count += 1

            return count

    def finalize(self):
        with self._condition:
            self._final = True
            self._condition.notify_all()

    def complete(self, block=True):
        with self._condition:
            while True:
                if self._final and \
                        len(self._unscheduled_transactions()) == 0:
                    return True

                if block:
                    self._condition.wait()
                else:
                    return False

    def __del__(self):
        self.cancel()

    def __iter__(self):
        return SchedulerIterator(self, self._condition)

    def count(self):
        with self._condition:
            return len(self._scheduled)

    def get_transaction(self, index):
        with self._condition:
            return self._scheduled_txn_info[self._scheduled[index]]

    def cancel(self):
        with self._condition:
            self._cancelled = True
            self._condition.notify_all()

    def is_cancelled(self):
        with self._condition:
            return self._cancelled
